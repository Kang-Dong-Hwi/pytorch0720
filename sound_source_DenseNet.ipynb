{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "sound_source_DenseNet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BOn5xCyhP_c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eec6ff57-296f-4248-f7d4-aa645c53ed68"
      },
      "source": [
        "\n",
        "import torch\n",
        "import os\n",
        "import random\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "import librosa\n",
        "import librosa.display\n",
        "import numba.decorators\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from numba.decorators import jit as optional_jit\n",
        "\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "\n",
        "#PATH = 'C://Projects//keras_talk//keras//intern//dataset//'\n",
        "PATH = '/content/gdrive/My Drive/dataset/'\n",
        "\n",
        "train_size = 800\n",
        "test_size = 200\n",
        "\n",
        "EPOCHS = 10\n",
        "BATCH_SIZE = 40\n",
        "\n",
        "\n",
        "def Y_DATA(y_data):\n",
        "    for idx in range(y_data.shape[0]):\n",
        "        y = y_data[idx]\n",
        "        if y < 0:  y_data[idx] = 10\n",
        "        else:      y_data[idx] = (y//20)\n",
        "    return y_data"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLBucxEyDgyf",
        "colab_type": "text"
      },
      "source": [
        "###### data normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Rk41xrd0XNHq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "992d8295-0d63-49b8-9f6f-4cabbd8ae004"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "dataset_dict = { 0 : 'S_left',        1 : 'S_left_phase',\n",
        "                 2 : 'S_right',       3 : 'S_right_phase',\n",
        "                 4 : 'clean_left',    5 : 'clean_left_phase',\n",
        "                 6 : 'clean_right',   7 : 'clean_right_phase',\n",
        "                 8 : 'idx_drone_end', 9 : 'idx_voice_end',\n",
        "                10 : 'idx_voice_start'}\n",
        "\n",
        "\n",
        "x_data_list = [0,2,1,3]\n",
        "\n",
        "\n",
        "numpy_dict = dict()\n",
        "for n in x_data_list:\n",
        "    numpy_name    = dataset_dict[n]\n",
        "    numpy_dict[n] = np.load( PATH + numpy_name + '.npy' )\n",
        "    \n",
        "\n",
        "\n",
        "'''    x_data,       y_data '''\n",
        "'''(1000,4,257,382), (1000,)'''\n",
        "\n",
        "x_data = []\n",
        "for idx in range(1000):\n",
        "    x_element = []\n",
        "\n",
        "    for n in x_data_list:\n",
        "        x_element.append( numpy_dict[n][:,:,idx] )\n",
        "\n",
        "    x_element = np.asarray( x_element )\n",
        "    \n",
        "    \n",
        "    # log scale 변환 [dB]\n",
        "    x_element[0] = 20*np.log10( np.abs(x_element[0]) + np.finfo(np.float32).eps )\n",
        "    x_element[1] = 20*np.log10( np.abs(x_element[1]) + np.finfo(np.float32).eps )\n",
        "\n",
        "\n",
        "    #magnitude normalization\n",
        "    x_mean = x_element[0].mean()\n",
        "    x_stdv = x_element[0].std()\n",
        "    x_element[0] = ( x_element[0] - x_mean ) / x_stdv\n",
        "\n",
        "    x_mean = x_element[1].mean()\n",
        "    x_stdv = x_element[1].std()\n",
        "    x_element[1] = ( x_element[1] - x_mean ) / x_stdv\n",
        "\n",
        "\n",
        "    #phase normalization\n",
        "    x_mean = x_element[0].mean()\n",
        "    x_stdv = x_element[0].std()\n",
        "    x_element[2] = ( x_element[0] - x_mean ) / x_stdv\n",
        "\n",
        "    x_mean = x_element[1].mean()\n",
        "    x_stdv = x_element[1].std()\n",
        "    x_element[3] = ( x_element[1] - x_mean ) / x_stdv\n",
        "\n",
        "\n",
        "    x_data.append( x_element )\n",
        "\n",
        "\n",
        "x_data = np.asarray(x_data)\n",
        "y_data = Y_DATA( np.load(PATH + 'angle.npy') )\n",
        "print('done..')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done..\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "colab_type": "code",
        "id": "oH6kABe5XhZT",
        "colab": {}
      },
      "source": [
        "\n",
        "#x_data = x_data.reshape()\n",
        "#y_data = y_data.reshape()\n",
        "\n",
        "\n",
        "\n",
        "x_data = torch.from_numpy( x_data ).float().to('cuda')\n",
        "y_data = torch.from_numpy( y_data ).long().to('cuda')\n",
        "\n",
        "full_dataset = TensorDataset( x_data, y_data )\n",
        "\n",
        "\n",
        "train_dataset, valid_dataset = torch.utils.data.random_split( full_dataset, [train_size, test_size])\n",
        "train_dataset = DataLoader( dataset=train_dataset, batch_size = BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "valid_dataset = DataLoader( dataset=valid_dataset, batch_size = BATCH_SIZE, shuffle=True, drop_last=True)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PjH37Dc-iHv7"
      },
      "source": [
        "#### DenseNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3Qt8_mI1vY0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "#import torch.utils.checkpoint as cp\n",
        "from collections import OrderedDict\n",
        "from torch import Tensor\n",
        "#from torch.jit.annotations import List\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uf8k2LB5Ug83",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class _DenseLayer(nn.Module):\n",
        "    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate, memory_efficient=False):\n",
        "        super(_DenseLayer, self).__init__()\n",
        "        \n",
        "        self.add_module('norm1', nn.BatchNorm2d(num_input_features)),\n",
        "        self.add_module('relu1', nn.ReLU(inplace=True)),\n",
        "        self.add_module('conv1', nn.Conv2d(num_input_features, bn_size * growth_rate,\n",
        "                                           kernel_size=1, stride=1,bias=False)),\n",
        "        \n",
        "        self.add_module('norm2', nn.BatchNorm2d(bn_size * growth_rate)),\n",
        "        self.add_module('relu2', nn.ReLU(inplace=True)),\n",
        "        self.add_module('conv2', nn.Conv2d(bn_size * growth_rate, growth_rate,\n",
        "                                           kernel_size=3, stride=1, padding=1,\n",
        "                                           bias=False)),\n",
        "        \n",
        "        self.drop_rate = float(drop_rate)\n",
        "        self.memory_efficient = memory_efficient\n",
        "\n",
        "    def bn_function(self, inputs):\n",
        "        # type: (List[Tensor]) -> Tensor\n",
        "        concated_features = torch.cat(inputs, 1)\n",
        "        bottleneck_output = self.conv1(self.relu1(self.norm1(concated_features)))  # noqa: T484\n",
        "        \n",
        "        return bottleneck_output\n",
        "\n",
        "\n",
        "    def forward(self, input):  # noqa: F811\n",
        "        if isinstance(input, Tensor):\n",
        "            prev_features = [input]\n",
        "        else:\n",
        "            prev_features = input\n",
        "\n",
        "\n",
        "        bottleneck_output = self.bn_function(prev_features)\n",
        "        new_features = self.conv2(self.relu2(self.norm2(bottleneck_output)))\n",
        "        \n",
        "        if self.drop_rate > 0:\n",
        "            new_features = F.dropout(new_features, p=self.drop_rate,\n",
        "                                     training=self.training)\n",
        "        \n",
        "        return new_features\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zj_EnuGyUhfg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class _DenseBlock(nn.ModuleDict):\n",
        "    _version = 2\n",
        "\n",
        "    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate, memory_efficient=False):\n",
        "        super(_DenseBlock, self).__init__()\n",
        "        for i in range(num_layers):\n",
        "            layer = _DenseLayer(\n",
        "                num_input_features + i * growth_rate,\n",
        "                growth_rate=growth_rate,\n",
        "                bn_size=bn_size,\n",
        "                drop_rate=drop_rate,\n",
        "                memory_efficient=memory_efficient,\n",
        "            )\n",
        "            self.add_module('denselayer%d' % (i + 1), layer)\n",
        "\n",
        "    def forward(self, init_features):\n",
        "        features = [init_features]\n",
        "        for name, layer in self.items():\n",
        "            new_features = layer(features)\n",
        "            features.append(new_features)\n",
        "        return torch.cat(features, 1)\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGPf02PMUh4E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "class _Transition(nn.Sequential):\n",
        "    def __init__(self, num_input_features, num_output_features):\n",
        "        super(_Transition, self).__init__()\n",
        "        self.add_module('norm', nn.BatchNorm2d(num_input_features))\n",
        "        self.add_module('relu', nn.ReLU(inplace=True))\n",
        "        self.add_module('conv', nn.Conv2d(num_input_features,\n",
        "                                          num_output_features,\n",
        "                                          kernel_size=1, stride=1, bias=False))\n",
        "        self.add_module('pool', nn.AvgPool2d(kernel_size=2, stride=2))\n",
        "\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26XC_s-lUtwF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "class DenseNet(nn.Module):\n",
        "    '''growth_rate, drop_rate'''\n",
        "    def __init__(self, growth_rate=20, block_config=(6, 12, 24, 16),\n",
        "                 num_init_features=128, bn_size=4, drop_rate=0.10,\n",
        "                 num_classes=11, memory_efficient=False):\n",
        "\n",
        "        super(DenseNet, self).__init__()\n",
        "\n",
        "        # First convolution\n",
        "        self.features = nn.Sequential(OrderedDict([\n",
        "            ('conv0', nn.Conv2d(4, num_init_features, kernel_size=7, stride=2, padding=3, bias=False)),\n",
        "            ('norm0', nn.BatchNorm2d(num_init_features)),\n",
        "            ('relu0', nn.ReLU(inplace=True)),\n",
        "            ('pool0', nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\n",
        "        ]))\n",
        "\n",
        "\n",
        "        # Each denseblock\n",
        "        num_features = num_init_features\n",
        "        for i, num_layers in enumerate(block_config):\n",
        "            block = _DenseBlock(\n",
        "                num_layers=num_layers,\n",
        "                num_input_features=num_features,\n",
        "                bn_size=bn_size,\n",
        "                growth_rate=growth_rate,\n",
        "                drop_rate=drop_rate,\n",
        "                memory_efficient=memory_efficient\n",
        "            )\n",
        "            self.features.add_module('denseblock%d' % (i + 1), block)\n",
        "            num_features = num_features + num_layers * growth_rate\n",
        "\n",
        "            if i != len(block_config) - 1:\n",
        "                trans = _Transition(num_input_features=num_features,\n",
        "                                    num_output_features=num_features // 2)\n",
        "                self.features.add_module('transition%d' % (i + 1), trans)\n",
        "                num_features = num_features // 2\n",
        "\n",
        "        # Final batch norm\n",
        "        self.features.add_module('norm5', nn.BatchNorm2d(num_features))\n",
        "\n",
        "        # Linear layer\n",
        "        self.classifier = nn.Linear(num_features, num_classes)\n",
        "\n",
        "        # Official init from torch repo.\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.features(x)\n",
        "        out = F.relu(features, inplace=True)\n",
        "        out = F.adaptive_avg_pool2d(out, (1, 1))\n",
        "        out = torch.flatten(out, 1)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1EZpcTrU3qh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "def _densenet(arch, growth_rate, block_config, num_init_features, pretrained, progress, **kwargs):\n",
        "    return DenseNet(growth_rate, block_config, num_init_features, **kwargs)\n",
        "\n",
        "\n",
        "\n",
        "def densenet_custom(pretrained=False, progress=True, **kwargs):\n",
        "    return _densenet('densenet_custom', 20, (8, 8, 4), 32, pretrained, progress, **kwargs)\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8hnA7UlDTeG",
        "colab_type": "text"
      },
      "source": [
        "##### model train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCRi61pl6_pY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "598b58aa-d069-45dc-86a8-515e67fe458a"
      },
      "source": [
        "torch.manual_seed(1)\n",
        "\n",
        "\n",
        "model = densenet_custom().to('cuda')\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().to('cuda')\n",
        "optimizer = torch.optim.Adagrad(model.parameters(), lr=0.00001, weight_decay=0.9)\n",
        "\n",
        "\n",
        "train_loss = []\n",
        "train_acc  = []\n",
        "\n",
        "\n",
        "model.train()\n",
        "for epoch in range(30):\n",
        "    print('epoch' + str(epoch+1))\n",
        "    \n",
        "    for i, (data, label) in enumerate(train_dataset):\n",
        "        (data, label) = (data.to('cuda'), label.to('cuda'))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "  \n",
        "        loss = F.nll_loss(output, label.reshape(BATCH_SIZE))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        preds = output.data.max(1)[1]\n",
        "        corr  = (preds==label.reshape(BATCH_SIZE)).sum().item()\n",
        "        acc   = corr/BATCH_SIZE*100\n",
        "        \n",
        "        train_loss.append(loss.item())\n",
        "        train_acc.append( acc )\n",
        "        \n",
        "        print('\\tLoss: {:.3f}\\tAcc: {:.3f}'.format(loss.item(), acc))\n",
        "        "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch1\n",
            "\tLoss: -0.078\tAcc: 12.500\n",
            "\tLoss: -0.080\tAcc: 7.500\n",
            "\tLoss: -0.034\tAcc: 12.500\n",
            "\tLoss: -0.122\tAcc: 20.000\n",
            "\tLoss: -0.100\tAcc: 5.000\n",
            "\tLoss: 0.014\tAcc: 0.000\n",
            "\tLoss: -0.029\tAcc: 7.500\n",
            "\tLoss: 0.025\tAcc: 2.500\n",
            "\tLoss: -0.071\tAcc: 10.000\n",
            "\tLoss: -0.018\tAcc: 7.500\n",
            "\tLoss: -0.064\tAcc: 2.500\n",
            "\tLoss: -0.091\tAcc: 5.000\n",
            "\tLoss: -0.037\tAcc: 7.500\n",
            "\tLoss: -0.122\tAcc: 15.000\n",
            "\tLoss: -0.028\tAcc: 2.500\n",
            "\tLoss: -0.032\tAcc: 5.000\n",
            "\tLoss: -0.091\tAcc: 10.000\n",
            "\tLoss: -0.052\tAcc: 15.000\n",
            "\tLoss: -0.120\tAcc: 10.000\n",
            "\tLoss: -0.100\tAcc: 12.500\n",
            "epoch2\n",
            "\tLoss: -0.071\tAcc: 10.000\n",
            "\tLoss: -0.029\tAcc: 2.500\n",
            "\tLoss: -0.092\tAcc: 10.000\n",
            "\tLoss: -0.067\tAcc: 2.500\n",
            "\tLoss: -0.083\tAcc: 12.500\n",
            "\tLoss: -0.038\tAcc: 5.000\n",
            "\tLoss: -0.071\tAcc: 10.000\n",
            "\tLoss: -0.139\tAcc: 22.500\n",
            "\tLoss: -0.019\tAcc: 5.000\n",
            "\tLoss: -0.050\tAcc: 7.500\n",
            "\tLoss: -0.093\tAcc: 7.500\n",
            "\tLoss: -0.111\tAcc: 10.000\n",
            "\tLoss: -0.070\tAcc: 5.000\n",
            "\tLoss: -0.035\tAcc: 5.000\n",
            "\tLoss: -0.059\tAcc: 0.000\n",
            "\tLoss: -0.041\tAcc: 7.500\n",
            "\tLoss: -0.024\tAcc: 15.000\n",
            "\tLoss: -0.123\tAcc: 15.000\n",
            "\tLoss: -0.047\tAcc: 5.000\n",
            "\tLoss: -0.030\tAcc: 10.000\n",
            "epoch3\n",
            "\tLoss: -0.083\tAcc: 7.500\n",
            "\tLoss: -0.095\tAcc: 7.500\n",
            "\tLoss: -0.081\tAcc: 10.000\n",
            "\tLoss: -0.056\tAcc: 12.500\n",
            "\tLoss: -0.190\tAcc: 15.000\n",
            "\tLoss: -0.077\tAcc: 12.500\n",
            "\tLoss: -0.031\tAcc: 2.500\n",
            "\tLoss: 0.061\tAcc: 0.000\n",
            "\tLoss: -0.064\tAcc: 5.000\n",
            "\tLoss: 0.044\tAcc: 0.000\n",
            "\tLoss: 0.016\tAcc: 10.000\n",
            "\tLoss: -0.116\tAcc: 7.500\n",
            "\tLoss: -0.067\tAcc: 7.500\n",
            "\tLoss: -0.066\tAcc: 2.500\n",
            "\tLoss: -0.071\tAcc: 7.500\n",
            "\tLoss: -0.062\tAcc: 15.000\n",
            "\tLoss: -0.059\tAcc: 10.000\n",
            "\tLoss: -0.059\tAcc: 5.000\n",
            "\tLoss: -0.124\tAcc: 15.000\n",
            "\tLoss: -0.108\tAcc: 12.500\n",
            "epoch4\n",
            "\tLoss: -0.099\tAcc: 12.500\n",
            "\tLoss: -0.085\tAcc: 5.000\n",
            "\tLoss: -0.014\tAcc: 5.000\n",
            "\tLoss: -0.023\tAcc: 7.500\n",
            "\tLoss: -0.034\tAcc: 10.000\n",
            "\tLoss: -0.061\tAcc: 2.500\n",
            "\tLoss: -0.072\tAcc: 12.500\n",
            "\tLoss: -0.081\tAcc: 12.500\n",
            "\tLoss: -0.115\tAcc: 5.000\n",
            "\tLoss: -0.062\tAcc: 10.000\n",
            "\tLoss: -0.086\tAcc: 7.500\n",
            "\tLoss: -0.084\tAcc: 10.000\n",
            "\tLoss: -0.129\tAcc: 7.500\n",
            "\tLoss: -0.123\tAcc: 7.500\n",
            "\tLoss: 0.013\tAcc: 2.500\n",
            "\tLoss: -0.106\tAcc: 17.500\n",
            "\tLoss: -0.085\tAcc: 7.500\n",
            "\tLoss: -0.046\tAcc: 7.500\n",
            "\tLoss: -0.018\tAcc: 7.500\n",
            "\tLoss: -0.009\tAcc: 5.000\n",
            "epoch5\n",
            "\tLoss: -0.065\tAcc: 10.000\n",
            "\tLoss: -0.048\tAcc: 7.500\n",
            "\tLoss: -0.011\tAcc: 10.000\n",
            "\tLoss: -0.093\tAcc: 5.000\n",
            "\tLoss: -0.145\tAcc: 10.000\n",
            "\tLoss: -0.080\tAcc: 10.000\n",
            "\tLoss: -0.138\tAcc: 15.000\n",
            "\tLoss: -0.064\tAcc: 12.500\n",
            "\tLoss: -0.101\tAcc: 7.500\n",
            "\tLoss: -0.016\tAcc: 12.500\n",
            "\tLoss: 0.002\tAcc: 2.500\n",
            "\tLoss: -0.037\tAcc: 2.500\n",
            "\tLoss: -0.059\tAcc: 12.500\n",
            "\tLoss: -0.090\tAcc: 5.000\n",
            "\tLoss: -0.035\tAcc: 7.500\n",
            "\tLoss: -0.041\tAcc: 2.500\n",
            "\tLoss: -0.065\tAcc: 7.500\n",
            "\tLoss: -0.077\tAcc: 7.500\n",
            "\tLoss: -0.107\tAcc: 12.500\n",
            "\tLoss: -0.065\tAcc: 10.000\n",
            "epoch6\n",
            "\tLoss: -0.140\tAcc: 10.000\n",
            "\tLoss: -0.117\tAcc: 10.000\n",
            "\tLoss: -0.083\tAcc: 2.500\n",
            "\tLoss: -0.105\tAcc: 17.500\n",
            "\tLoss: -0.065\tAcc: 15.000\n",
            "\tLoss: -0.061\tAcc: 7.500\n",
            "\tLoss: -0.106\tAcc: 10.000\n",
            "\tLoss: -0.028\tAcc: 5.000\n",
            "\tLoss: 0.056\tAcc: 0.000\n",
            "\tLoss: -0.022\tAcc: 2.500\n",
            "\tLoss: -0.069\tAcc: 12.500\n",
            "\tLoss: -0.033\tAcc: 0.000\n",
            "\tLoss: -0.023\tAcc: 7.500\n",
            "\tLoss: -0.029\tAcc: 5.000\n",
            "\tLoss: -0.035\tAcc: 12.500\n",
            "\tLoss: -0.062\tAcc: 12.500\n",
            "\tLoss: -0.041\tAcc: 15.000\n",
            "\tLoss: -0.165\tAcc: 15.000\n",
            "\tLoss: -0.075\tAcc: 7.500\n",
            "\tLoss: -0.145\tAcc: 12.500\n",
            "epoch7\n",
            "\tLoss: -0.098\tAcc: 10.000\n",
            "\tLoss: -0.168\tAcc: 12.500\n",
            "\tLoss: -0.098\tAcc: 10.000\n",
            "\tLoss: -0.050\tAcc: 2.500\n",
            "\tLoss: -0.051\tAcc: 7.500\n",
            "\tLoss: -0.135\tAcc: 7.500\n",
            "\tLoss: -0.064\tAcc: 7.500\n",
            "\tLoss: -0.052\tAcc: 2.500\n",
            "\tLoss: -0.045\tAcc: 12.500\n",
            "\tLoss: -0.059\tAcc: 10.000\n",
            "\tLoss: 0.010\tAcc: 5.000\n",
            "\tLoss: -0.111\tAcc: 10.000\n",
            "\tLoss: -0.143\tAcc: 12.500\n",
            "\tLoss: 0.011\tAcc: 5.000\n",
            "\tLoss: -0.039\tAcc: 10.000\n",
            "\tLoss: -0.085\tAcc: 5.000\n",
            "\tLoss: -0.046\tAcc: 10.000\n",
            "\tLoss: -0.004\tAcc: 7.500\n",
            "\tLoss: -0.021\tAcc: 10.000\n",
            "\tLoss: -0.111\tAcc: 12.500\n",
            "epoch8\n",
            "\tLoss: 0.018\tAcc: 0.000\n",
            "\tLoss: -0.120\tAcc: 10.000\n",
            "\tLoss: -0.046\tAcc: 10.000\n",
            "\tLoss: -0.114\tAcc: 12.500\n",
            "\tLoss: -0.108\tAcc: 2.500\n",
            "\tLoss: -0.053\tAcc: 10.000\n",
            "\tLoss: -0.147\tAcc: 20.000\n",
            "\tLoss: 0.015\tAcc: 5.000\n",
            "\tLoss: -0.063\tAcc: 12.500\n",
            "\tLoss: -0.025\tAcc: 2.500\n",
            "\tLoss: -0.025\tAcc: 10.000\n",
            "\tLoss: -0.070\tAcc: 10.000\n",
            "\tLoss: -0.056\tAcc: 5.000\n",
            "\tLoss: -0.117\tAcc: 15.000\n",
            "\tLoss: -0.072\tAcc: 5.000\n",
            "\tLoss: -0.063\tAcc: 5.000\n",
            "\tLoss: -0.072\tAcc: 2.500\n",
            "\tLoss: -0.059\tAcc: 5.000\n",
            "\tLoss: -0.042\tAcc: 10.000\n",
            "\tLoss: -0.145\tAcc: 20.000\n",
            "epoch9\n",
            "\tLoss: -0.059\tAcc: 5.000\n",
            "\tLoss: -0.092\tAcc: 5.000\n",
            "\tLoss: -0.048\tAcc: 7.500\n",
            "\tLoss: -0.083\tAcc: 5.000\n",
            "\tLoss: -0.138\tAcc: 12.500\n",
            "\tLoss: -0.106\tAcc: 15.000\n",
            "\tLoss: -0.080\tAcc: 7.500\n",
            "\tLoss: 0.004\tAcc: 10.000\n",
            "\tLoss: -0.108\tAcc: 17.500\n",
            "\tLoss: -0.008\tAcc: 5.000\n",
            "\tLoss: -0.123\tAcc: 7.500\n",
            "\tLoss: -0.057\tAcc: 10.000\n",
            "\tLoss: -0.021\tAcc: 5.000\n",
            "\tLoss: -0.093\tAcc: 7.500\n",
            "\tLoss: -0.051\tAcc: 7.500\n",
            "\tLoss: -0.053\tAcc: 2.500\n",
            "\tLoss: -0.118\tAcc: 12.500\n",
            "\tLoss: -0.018\tAcc: 5.000\n",
            "\tLoss: -0.056\tAcc: 10.000\n",
            "\tLoss: -0.094\tAcc: 5.000\n",
            "epoch10\n",
            "\tLoss: -0.092\tAcc: 5.000\n",
            "\tLoss: -0.084\tAcc: 10.000\n",
            "\tLoss: 0.011\tAcc: 10.000\n",
            "\tLoss: 0.010\tAcc: 17.500\n",
            "\tLoss: -0.061\tAcc: 2.500\n",
            "\tLoss: -0.046\tAcc: 10.000\n",
            "\tLoss: -0.070\tAcc: 2.500\n",
            "\tLoss: -0.078\tAcc: 5.000\n",
            "\tLoss: -0.101\tAcc: 7.500\n",
            "\tLoss: -0.009\tAcc: 2.500\n",
            "\tLoss: -0.083\tAcc: 7.500\n",
            "\tLoss: -0.086\tAcc: 5.000\n",
            "\tLoss: -0.151\tAcc: 10.000\n",
            "\tLoss: -0.089\tAcc: 15.000\n",
            "\tLoss: -0.093\tAcc: 12.500\n",
            "\tLoss: -0.092\tAcc: 7.500\n",
            "\tLoss: -0.055\tAcc: 10.000\n",
            "\tLoss: -0.158\tAcc: 10.000\n",
            "\tLoss: -0.003\tAcc: 5.000\n",
            "\tLoss: -0.079\tAcc: 5.000\n",
            "epoch11\n",
            "\tLoss: -0.015\tAcc: 5.000\n",
            "\tLoss: -0.051\tAcc: 2.500\n",
            "\tLoss: -0.101\tAcc: 10.000\n",
            "\tLoss: -0.026\tAcc: 7.500\n",
            "\tLoss: -0.016\tAcc: 5.000\n",
            "\tLoss: -0.104\tAcc: 5.000\n",
            "\tLoss: -0.028\tAcc: 10.000\n",
            "\tLoss: -0.077\tAcc: 7.500\n",
            "\tLoss: -0.100\tAcc: 15.000\n",
            "\tLoss: -0.160\tAcc: 7.500\n",
            "\tLoss: -0.087\tAcc: 15.000\n",
            "\tLoss: -0.048\tAcc: 5.000\n",
            "\tLoss: -0.057\tAcc: 7.500\n",
            "\tLoss: -0.089\tAcc: 15.000\n",
            "\tLoss: -0.096\tAcc: 5.000\n",
            "\tLoss: -0.115\tAcc: 5.000\n",
            "\tLoss: -0.047\tAcc: 7.500\n",
            "\tLoss: -0.082\tAcc: 12.500\n",
            "\tLoss: 0.012\tAcc: 0.000\n",
            "\tLoss: -0.133\tAcc: 15.000\n",
            "epoch12\n",
            "\tLoss: -0.122\tAcc: 10.000\n",
            "\tLoss: -0.087\tAcc: 15.000\n",
            "\tLoss: -0.133\tAcc: 12.500\n",
            "\tLoss: -0.000\tAcc: 10.000\n",
            "\tLoss: -0.056\tAcc: 2.500\n",
            "\tLoss: -0.042\tAcc: 7.500\n",
            "\tLoss: -0.111\tAcc: 15.000\n",
            "\tLoss: -0.089\tAcc: 7.500\n",
            "\tLoss: -0.072\tAcc: 5.000\n",
            "\tLoss: 0.001\tAcc: 7.500\n",
            "\tLoss: -0.056\tAcc: 2.500\n",
            "\tLoss: -0.080\tAcc: 5.000\n",
            "\tLoss: -0.085\tAcc: 0.000\n",
            "\tLoss: -0.102\tAcc: 12.500\n",
            "\tLoss: -0.035\tAcc: 10.000\n",
            "\tLoss: -0.071\tAcc: 0.000\n",
            "\tLoss: -0.161\tAcc: 17.500\n",
            "\tLoss: -0.008\tAcc: 2.500\n",
            "\tLoss: -0.056\tAcc: 7.500\n",
            "\tLoss: -0.063\tAcc: 7.500\n",
            "epoch13\n",
            "\tLoss: -0.074\tAcc: 15.000\n",
            "\tLoss: -0.097\tAcc: 7.500\n",
            "\tLoss: -0.043\tAcc: 0.000\n",
            "\tLoss: -0.060\tAcc: 15.000\n",
            "\tLoss: -0.047\tAcc: 12.500\n",
            "\tLoss: -0.153\tAcc: 5.000\n",
            "\tLoss: -0.176\tAcc: 10.000\n",
            "\tLoss: -0.105\tAcc: 10.000\n",
            "\tLoss: -0.067\tAcc: 20.000\n",
            "\tLoss: -0.069\tAcc: 5.000\n",
            "\tLoss: -0.082\tAcc: 5.000\n",
            "\tLoss: -0.038\tAcc: 5.000\n",
            "\tLoss: -0.115\tAcc: 12.500\n",
            "\tLoss: -0.099\tAcc: 12.500\n",
            "\tLoss: -0.066\tAcc: 15.000\n",
            "\tLoss: 0.019\tAcc: 2.500\n",
            "\tLoss: -0.023\tAcc: 5.000\n",
            "\tLoss: -0.065\tAcc: 5.000\n",
            "\tLoss: -0.056\tAcc: 10.000\n",
            "\tLoss: -0.010\tAcc: 5.000\n",
            "epoch14\n",
            "\tLoss: -0.058\tAcc: 2.500\n",
            "\tLoss: -0.049\tAcc: 10.000\n",
            "\tLoss: -0.054\tAcc: 5.000\n",
            "\tLoss: -0.064\tAcc: 5.000\n",
            "\tLoss: -0.012\tAcc: 7.500\n",
            "\tLoss: -0.106\tAcc: 7.500\n",
            "\tLoss: -0.061\tAcc: 10.000\n",
            "\tLoss: -0.068\tAcc: 10.000\n",
            "\tLoss: 0.005\tAcc: 7.500\n",
            "\tLoss: -0.124\tAcc: 7.500\n",
            "\tLoss: -0.056\tAcc: 10.000\n",
            "\tLoss: -0.088\tAcc: 2.500\n",
            "\tLoss: -0.093\tAcc: 7.500\n",
            "\tLoss: -0.082\tAcc: 2.500\n",
            "\tLoss: -0.085\tAcc: 7.500\n",
            "\tLoss: -0.107\tAcc: 12.500\n",
            "\tLoss: -0.133\tAcc: 15.000\n",
            "\tLoss: -0.060\tAcc: 7.500\n",
            "\tLoss: -0.070\tAcc: 10.000\n",
            "\tLoss: -0.088\tAcc: 12.500\n",
            "epoch15\n",
            "\tLoss: -0.161\tAcc: 15.000\n",
            "\tLoss: -0.035\tAcc: 0.000\n",
            "\tLoss: -0.077\tAcc: 15.000\n",
            "\tLoss: -0.025\tAcc: 7.500\n",
            "\tLoss: -0.072\tAcc: 5.000\n",
            "\tLoss: -0.135\tAcc: 12.500\n",
            "\tLoss: -0.051\tAcc: 12.500\n",
            "\tLoss: -0.050\tAcc: 5.000\n",
            "\tLoss: -0.031\tAcc: 2.500\n",
            "\tLoss: -0.036\tAcc: 10.000\n",
            "\tLoss: -0.082\tAcc: 12.500\n",
            "\tLoss: -0.081\tAcc: 10.000\n",
            "\tLoss: -0.068\tAcc: 12.500\n",
            "\tLoss: -0.041\tAcc: 7.500\n",
            "\tLoss: -0.052\tAcc: 5.000\n",
            "\tLoss: -0.128\tAcc: 10.000\n",
            "\tLoss: -0.100\tAcc: 7.500\n",
            "\tLoss: -0.046\tAcc: 10.000\n",
            "\tLoss: -0.053\tAcc: 7.500\n",
            "\tLoss: -0.123\tAcc: 10.000\n",
            "epoch16\n",
            "\tLoss: -0.164\tAcc: 22.500\n",
            "\tLoss: -0.069\tAcc: 2.500\n",
            "\tLoss: -0.096\tAcc: 5.000\n",
            "\tLoss: -0.058\tAcc: 7.500\n",
            "\tLoss: -0.024\tAcc: 0.000\n",
            "\tLoss: -0.092\tAcc: 12.500\n",
            "\tLoss: 0.042\tAcc: 5.000\n",
            "\tLoss: -0.088\tAcc: 7.500\n",
            "\tLoss: -0.117\tAcc: 10.000\n",
            "\tLoss: -0.167\tAcc: 17.500\n",
            "\tLoss: -0.119\tAcc: 10.000\n",
            "\tLoss: -0.027\tAcc: 10.000\n",
            "\tLoss: -0.127\tAcc: 17.500\n",
            "\tLoss: -0.088\tAcc: 15.000\n",
            "\tLoss: -0.092\tAcc: 10.000\n",
            "\tLoss: 0.006\tAcc: 2.500\n",
            "\tLoss: 0.009\tAcc: 7.500\n",
            "\tLoss: -0.008\tAcc: 5.000\n",
            "\tLoss: -0.126\tAcc: 10.000\n",
            "\tLoss: -0.063\tAcc: 2.500\n",
            "epoch17\n",
            "\tLoss: -0.029\tAcc: 5.000\n",
            "\tLoss: -0.066\tAcc: 10.000\n",
            "\tLoss: -0.120\tAcc: 7.500\n",
            "\tLoss: -0.100\tAcc: 10.000\n",
            "\tLoss: -0.088\tAcc: 0.000\n",
            "\tLoss: -0.035\tAcc: 5.000\n",
            "\tLoss: -0.124\tAcc: 10.000\n",
            "\tLoss: -0.073\tAcc: 20.000\n",
            "\tLoss: -0.099\tAcc: 10.000\n",
            "\tLoss: -0.086\tAcc: 2.500\n",
            "\tLoss: -0.053\tAcc: 10.000\n",
            "\tLoss: -0.119\tAcc: 7.500\n",
            "\tLoss: -0.040\tAcc: 7.500\n",
            "\tLoss: -0.048\tAcc: 5.000\n",
            "\tLoss: -0.097\tAcc: 7.500\n",
            "\tLoss: -0.066\tAcc: 2.500\n",
            "\tLoss: -0.073\tAcc: 17.500\n",
            "\tLoss: -0.152\tAcc: 12.500\n",
            "\tLoss: 0.003\tAcc: 5.000\n",
            "\tLoss: -0.035\tAcc: 5.000\n",
            "epoch18\n",
            "\tLoss: -0.013\tAcc: 2.500\n",
            "\tLoss: -0.136\tAcc: 7.500\n",
            "\tLoss: -0.191\tAcc: 20.000\n",
            "\tLoss: -0.088\tAcc: 10.000\n",
            "\tLoss: -0.128\tAcc: 5.000\n",
            "\tLoss: -0.022\tAcc: 2.500\n",
            "\tLoss: -0.101\tAcc: 5.000\n",
            "\tLoss: -0.051\tAcc: 5.000\n",
            "\tLoss: -0.097\tAcc: 12.500\n",
            "\tLoss: -0.102\tAcc: 12.500\n",
            "\tLoss: -0.054\tAcc: 12.500\n",
            "\tLoss: -0.123\tAcc: 10.000\n",
            "\tLoss: -0.027\tAcc: 7.500\n",
            "\tLoss: -0.058\tAcc: 12.500\n",
            "\tLoss: -0.024\tAcc: 15.000\n",
            "\tLoss: -0.063\tAcc: 7.500\n",
            "\tLoss: -0.011\tAcc: 7.500\n",
            "\tLoss: -0.086\tAcc: 5.000\n",
            "\tLoss: -0.103\tAcc: 10.000\n",
            "\tLoss: -0.020\tAcc: 5.000\n",
            "epoch19\n",
            "\tLoss: -0.065\tAcc: 7.500\n",
            "\tLoss: -0.064\tAcc: 7.500\n",
            "\tLoss: -0.028\tAcc: 10.000\n",
            "\tLoss: -0.148\tAcc: 10.000\n",
            "\tLoss: -0.073\tAcc: 7.500\n",
            "\tLoss: -0.037\tAcc: 2.500\n",
            "\tLoss: -0.047\tAcc: 5.000\n",
            "\tLoss: -0.129\tAcc: 12.500\n",
            "\tLoss: -0.041\tAcc: 12.500\n",
            "\tLoss: -0.064\tAcc: 10.000\n",
            "\tLoss: -0.073\tAcc: 12.500\n",
            "\tLoss: 0.003\tAcc: 0.000\n",
            "\tLoss: -0.049\tAcc: 10.000\n",
            "\tLoss: -0.150\tAcc: 17.500\n",
            "\tLoss: -0.073\tAcc: 5.000\n",
            "\tLoss: -0.072\tAcc: 5.000\n",
            "\tLoss: -0.031\tAcc: 2.500\n",
            "\tLoss: -0.088\tAcc: 15.000\n",
            "\tLoss: -0.108\tAcc: 12.500\n",
            "\tLoss: -0.141\tAcc: 10.000\n",
            "epoch20\n",
            "\tLoss: -0.020\tAcc: 10.000\n",
            "\tLoss: -0.132\tAcc: 10.000\n",
            "\tLoss: -0.058\tAcc: 7.500\n",
            "\tLoss: -0.081\tAcc: 7.500\n",
            "\tLoss: -0.049\tAcc: 10.000\n",
            "\tLoss: -0.011\tAcc: 5.000\n",
            "\tLoss: -0.095\tAcc: 10.000\n",
            "\tLoss: -0.125\tAcc: 12.500\n",
            "\tLoss: -0.055\tAcc: 10.000\n",
            "\tLoss: -0.078\tAcc: 5.000\n",
            "\tLoss: -0.114\tAcc: 10.000\n",
            "\tLoss: -0.074\tAcc: 2.500\n",
            "\tLoss: 0.003\tAcc: 5.000\n",
            "\tLoss: -0.020\tAcc: 7.500\n",
            "\tLoss: -0.114\tAcc: 15.000\n",
            "\tLoss: -0.082\tAcc: 10.000\n",
            "\tLoss: -0.097\tAcc: 12.500\n",
            "\tLoss: -0.098\tAcc: 15.000\n",
            "\tLoss: -0.129\tAcc: 5.000\n",
            "\tLoss: -0.072\tAcc: 10.000\n",
            "epoch21\n",
            "\tLoss: -0.062\tAcc: 7.500\n",
            "\tLoss: -0.032\tAcc: 5.000\n",
            "\tLoss: -0.108\tAcc: 10.000\n",
            "\tLoss: -0.043\tAcc: 15.000\n",
            "\tLoss: -0.041\tAcc: 5.000\n",
            "\tLoss: -0.091\tAcc: 10.000\n",
            "\tLoss: -0.019\tAcc: 2.500\n",
            "\tLoss: -0.004\tAcc: 7.500\n",
            "\tLoss: -0.082\tAcc: 5.000\n",
            "\tLoss: -0.164\tAcc: 12.500\n",
            "\tLoss: -0.046\tAcc: 5.000\n",
            "\tLoss: -0.117\tAcc: 10.000\n",
            "\tLoss: -0.123\tAcc: 10.000\n",
            "\tLoss: -0.104\tAcc: 5.000\n",
            "\tLoss: -0.031\tAcc: 5.000\n",
            "\tLoss: -0.135\tAcc: 20.000\n",
            "\tLoss: -0.079\tAcc: 7.500\n",
            "\tLoss: -0.111\tAcc: 12.500\n",
            "\tLoss: -0.054\tAcc: 15.000\n",
            "\tLoss: -0.068\tAcc: 7.500\n",
            "epoch22\n",
            "\tLoss: -0.105\tAcc: 12.500\n",
            "\tLoss: -0.113\tAcc: 7.500\n",
            "\tLoss: -0.038\tAcc: 12.500\n",
            "\tLoss: -0.138\tAcc: 2.500\n",
            "\tLoss: -0.137\tAcc: 5.000\n",
            "\tLoss: -0.067\tAcc: 5.000\n",
            "\tLoss: -0.074\tAcc: 5.000\n",
            "\tLoss: -0.068\tAcc: 10.000\n",
            "\tLoss: -0.084\tAcc: 12.500\n",
            "\tLoss: -0.105\tAcc: 7.500\n",
            "\tLoss: -0.038\tAcc: 10.000\n",
            "\tLoss: -0.070\tAcc: 12.500\n",
            "\tLoss: -0.079\tAcc: 10.000\n",
            "\tLoss: -0.099\tAcc: 7.500\n",
            "\tLoss: -0.063\tAcc: 12.500\n",
            "\tLoss: -0.050\tAcc: 5.000\n",
            "\tLoss: -0.036\tAcc: 5.000\n",
            "\tLoss: -0.049\tAcc: 7.500\n",
            "\tLoss: -0.050\tAcc: 10.000\n",
            "\tLoss: -0.079\tAcc: 2.500\n",
            "epoch23\n",
            "\tLoss: -0.098\tAcc: 12.500\n",
            "\tLoss: -0.034\tAcc: 10.000\n",
            "\tLoss: -0.088\tAcc: 7.500\n",
            "\tLoss: -0.104\tAcc: 5.000\n",
            "\tLoss: -0.097\tAcc: 12.500\n",
            "\tLoss: -0.113\tAcc: 7.500\n",
            "\tLoss: -0.139\tAcc: 12.500\n",
            "\tLoss: -0.041\tAcc: 5.000\n",
            "\tLoss: -0.076\tAcc: 12.500\n",
            "\tLoss: -0.076\tAcc: 7.500\n",
            "\tLoss: -0.095\tAcc: 0.000\n",
            "\tLoss: -0.082\tAcc: 5.000\n",
            "\tLoss: 0.020\tAcc: 0.000\n",
            "\tLoss: -0.041\tAcc: 7.500\n",
            "\tLoss: -0.102\tAcc: 0.000\n",
            "\tLoss: -0.064\tAcc: 10.000\n",
            "\tLoss: -0.080\tAcc: 12.500\n",
            "\tLoss: -0.053\tAcc: 7.500\n",
            "\tLoss: -0.105\tAcc: 15.000\n",
            "\tLoss: -0.058\tAcc: 15.000\n",
            "epoch24\n",
            "\tLoss: -0.079\tAcc: 10.000\n",
            "\tLoss: -0.079\tAcc: 10.000\n",
            "\tLoss: -0.050\tAcc: 2.500\n",
            "\tLoss: -0.044\tAcc: 7.500\n",
            "\tLoss: -0.066\tAcc: 5.000\n",
            "\tLoss: -0.071\tAcc: 12.500\n",
            "\tLoss: -0.178\tAcc: 12.500\n",
            "\tLoss: -0.041\tAcc: 10.000\n",
            "\tLoss: -0.022\tAcc: 7.500\n",
            "\tLoss: -0.131\tAcc: 10.000\n",
            "\tLoss: -0.016\tAcc: 7.500\n",
            "\tLoss: -0.045\tAcc: 2.500\n",
            "\tLoss: -0.174\tAcc: 10.000\n",
            "\tLoss: -0.110\tAcc: 7.500\n",
            "\tLoss: -0.076\tAcc: 5.000\n",
            "\tLoss: 0.014\tAcc: 2.500\n",
            "\tLoss: -0.131\tAcc: 20.000\n",
            "\tLoss: -0.048\tAcc: 5.000\n",
            "\tLoss: -0.072\tAcc: 17.500\n",
            "\tLoss: -0.123\tAcc: 7.500\n",
            "epoch25\n",
            "\tLoss: -0.051\tAcc: 7.500\n",
            "\tLoss: -0.051\tAcc: 10.000\n",
            "\tLoss: 0.001\tAcc: 2.500\n",
            "\tLoss: -0.100\tAcc: 5.000\n",
            "\tLoss: -0.147\tAcc: 15.000\n",
            "\tLoss: -0.031\tAcc: 7.500\n",
            "\tLoss: -0.077\tAcc: 2.500\n",
            "\tLoss: -0.097\tAcc: 7.500\n",
            "\tLoss: -0.119\tAcc: 10.000\n",
            "\tLoss: -0.145\tAcc: 7.500\n",
            "\tLoss: -0.023\tAcc: 7.500\n",
            "\tLoss: -0.004\tAcc: 7.500\n",
            "\tLoss: -0.059\tAcc: 7.500\n",
            "\tLoss: -0.106\tAcc: 5.000\n",
            "\tLoss: -0.152\tAcc: 15.000\n",
            "\tLoss: -0.056\tAcc: 17.500\n",
            "\tLoss: -0.137\tAcc: 17.500\n",
            "\tLoss: -0.006\tAcc: 5.000\n",
            "\tLoss: -0.105\tAcc: 7.500\n",
            "\tLoss: -0.067\tAcc: 2.500\n",
            "epoch26\n",
            "\tLoss: -0.072\tAcc: 10.000\n",
            "\tLoss: -0.156\tAcc: 7.500\n",
            "\tLoss: -0.037\tAcc: 10.000\n",
            "\tLoss: -0.034\tAcc: 5.000\n",
            "\tLoss: -0.143\tAcc: 10.000\n",
            "\tLoss: -0.106\tAcc: 7.500\n",
            "\tLoss: -0.105\tAcc: 5.000\n",
            "\tLoss: -0.157\tAcc: 7.500\n",
            "\tLoss: -0.062\tAcc: 5.000\n",
            "\tLoss: -0.065\tAcc: 7.500\n",
            "\tLoss: -0.040\tAcc: 5.000\n",
            "\tLoss: -0.078\tAcc: 17.500\n",
            "\tLoss: 0.039\tAcc: 12.500\n",
            "\tLoss: 0.013\tAcc: 7.500\n",
            "\tLoss: -0.053\tAcc: 10.000\n",
            "\tLoss: -0.128\tAcc: 12.500\n",
            "\tLoss: -0.129\tAcc: 5.000\n",
            "\tLoss: -0.105\tAcc: 10.000\n",
            "\tLoss: -0.127\tAcc: 7.500\n",
            "\tLoss: -0.005\tAcc: 7.500\n",
            "epoch27\n",
            "\tLoss: -0.102\tAcc: 5.000\n",
            "\tLoss: -0.105\tAcc: 17.500\n",
            "\tLoss: -0.075\tAcc: 2.500\n",
            "\tLoss: -0.079\tAcc: 5.000\n",
            "\tLoss: -0.064\tAcc: 10.000\n",
            "\tLoss: -0.104\tAcc: 10.000\n",
            "\tLoss: -0.081\tAcc: 7.500\n",
            "\tLoss: -0.101\tAcc: 5.000\n",
            "\tLoss: -0.083\tAcc: 17.500\n",
            "\tLoss: -0.045\tAcc: 2.500\n",
            "\tLoss: -0.119\tAcc: 12.500\n",
            "\tLoss: -0.103\tAcc: 12.500\n",
            "\tLoss: -0.036\tAcc: 7.500\n",
            "\tLoss: -0.104\tAcc: 10.000\n",
            "\tLoss: -0.049\tAcc: 12.500\n",
            "\tLoss: -0.076\tAcc: 10.000\n",
            "\tLoss: -0.061\tAcc: 5.000\n",
            "\tLoss: -0.079\tAcc: 7.500\n",
            "\tLoss: -0.045\tAcc: 5.000\n",
            "\tLoss: -0.045\tAcc: 12.500\n",
            "epoch28\n",
            "\tLoss: -0.112\tAcc: 2.500\n",
            "\tLoss: -0.156\tAcc: 5.000\n",
            "\tLoss: -0.026\tAcc: 0.000\n",
            "\tLoss: -0.079\tAcc: 5.000\n",
            "\tLoss: -0.056\tAcc: 7.500\n",
            "\tLoss: -0.130\tAcc: 17.500\n",
            "\tLoss: -0.046\tAcc: 10.000\n",
            "\tLoss: -0.058\tAcc: 2.500\n",
            "\tLoss: -0.048\tAcc: 7.500\n",
            "\tLoss: -0.060\tAcc: 2.500\n",
            "\tLoss: -0.054\tAcc: 7.500\n",
            "\tLoss: -0.080\tAcc: 15.000\n",
            "\tLoss: -0.092\tAcc: 15.000\n",
            "\tLoss: -0.113\tAcc: 12.500\n",
            "\tLoss: -0.062\tAcc: 10.000\n",
            "\tLoss: -0.032\tAcc: 5.000\n",
            "\tLoss: -0.126\tAcc: 12.500\n",
            "\tLoss: -0.077\tAcc: 10.000\n",
            "\tLoss: -0.085\tAcc: 10.000\n",
            "\tLoss: -0.063\tAcc: 12.500\n",
            "epoch29\n",
            "\tLoss: -0.037\tAcc: 5.000\n",
            "\tLoss: -0.163\tAcc: 10.000\n",
            "\tLoss: -0.033\tAcc: 7.500\n",
            "\tLoss: 0.000\tAcc: 5.000\n",
            "\tLoss: -0.156\tAcc: 7.500\n",
            "\tLoss: -0.060\tAcc: 5.000\n",
            "\tLoss: -0.132\tAcc: 12.500\n",
            "\tLoss: -0.110\tAcc: 7.500\n",
            "\tLoss: -0.163\tAcc: 22.500\n",
            "\tLoss: 0.014\tAcc: 0.000\n",
            "\tLoss: -0.117\tAcc: 7.500\n",
            "\tLoss: -0.079\tAcc: 20.000\n",
            "\tLoss: 0.005\tAcc: 5.000\n",
            "\tLoss: -0.094\tAcc: 10.000\n",
            "\tLoss: -0.080\tAcc: 7.500\n",
            "\tLoss: -0.087\tAcc: 10.000\n",
            "\tLoss: -0.064\tAcc: 10.000\n",
            "\tLoss: -0.075\tAcc: 5.000\n",
            "\tLoss: -0.061\tAcc: 5.000\n",
            "\tLoss: -0.088\tAcc: 5.000\n",
            "epoch30\n",
            "\tLoss: -0.120\tAcc: 15.000\n",
            "\tLoss: -0.050\tAcc: 2.500\n",
            "\tLoss: -0.001\tAcc: 5.000\n",
            "\tLoss: -0.120\tAcc: 12.500\n",
            "\tLoss: -0.034\tAcc: 7.500\n",
            "\tLoss: -0.017\tAcc: 5.000\n",
            "\tLoss: -0.036\tAcc: 7.500\n",
            "\tLoss: -0.037\tAcc: 5.000\n",
            "\tLoss: -0.053\tAcc: 15.000\n",
            "\tLoss: -0.093\tAcc: 10.000\n",
            "\tLoss: -0.086\tAcc: 7.500\n",
            "\tLoss: -0.150\tAcc: 7.500\n",
            "\tLoss: -0.230\tAcc: 12.500\n",
            "\tLoss: -0.069\tAcc: 15.000\n",
            "\tLoss: -0.139\tAcc: 7.500\n",
            "\tLoss: -0.095\tAcc: 5.000\n",
            "\tLoss: -0.035\tAcc: 7.500\n",
            "\tLoss: -0.123\tAcc: 10.000\n",
            "\tLoss: -0.046\tAcc: 0.000\n",
            "\tLoss: -0.031\tAcc: 7.500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEvbZA7v2f1l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "df8bc069-3388-44c8-b5e4-3dc842cdedff"
      },
      "source": [
        "model"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DenseNet(\n",
              "  (features): Sequential(\n",
              "    (conv0): Conv2d(4, 32, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "    (norm0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu0): ReLU(inplace=True)\n",
              "    (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (denseblock1): _DenseBlock(\n",
              "      (denselayer1): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(32, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(80, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer2): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(52, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(52, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(80, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer3): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(72, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(80, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer4): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(92, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(92, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(80, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer5): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(112, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(80, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer6): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(132, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(80, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer7): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(152, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(80, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer8): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(172, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(172, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(80, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (transition1): _Transition(\n",
              "      (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "    )\n",
              "    (denseblock2): _DenseBlock(\n",
              "      (denselayer1): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(96, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(80, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer2): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(116, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(116, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(80, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer3): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(136, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(80, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer4): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(156, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(156, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(80, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer5): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(176, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(176, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(80, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer6): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(196, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(196, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(80, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer7): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(216, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(80, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer8): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(236, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(236, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(80, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (transition2): _Transition(\n",
              "      (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "    )\n",
              "    (denseblock3): _DenseBlock(\n",
              "      (denselayer1): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(128, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(80, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer2): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(148, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(148, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(80, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer3): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(168, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(80, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer4): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(188, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(188, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(80, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (norm5): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (classifier): Linear(in_features=208, out_features=11, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    }
  ]
}